{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "import torch\n",
    "import torchvision.utils\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import src.dataPreparation.CreatePartial as create_partial\n",
    "import src.neuralNetworksArch.BstCnn as bst\n",
    "import src.utils.Visual as vis\n",
    "import src.utils.Checkpoint as ckp\n",
    "import src.utils.Metrics as metrics\n",
    "\n",
    "from src.config.Param import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROB_SAME = {\n",
    "    'partial_1' : [\n",
    "        0.328,\n",
    "        0.368,\n",
    "        0.304\n",
    "    ],\n",
    "    'partial_2' : [\n",
    "        0.352697095,\n",
    "        0.369294606,\n",
    "        0.278008299\n",
    "    ],\n",
    "    'partial_3' : [\n",
    "        0.26993865,\n",
    "        0.251533742,\n",
    "        0.260736196,\n",
    "        0.217791411\n",
    "    ]\n",
    "}\n",
    "\n",
    "PROB_DIFF = {\n",
    "    'partial_1' : [\n",
    "        0.336099585,\n",
    "        0.356846473,\n",
    "        0.307053942\n",
    "    ],\n",
    "    'partial_2' : [\n",
    "        0.334919691,\n",
    "        0.348007139,\n",
    "        0.317073171\n",
    "    ],\n",
    "    'partial_3' : [\n",
    "        0.246292135,\n",
    "        0.253033708,\n",
    "        0.261123596,\n",
    "        0.239550562\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = [\n",
    "    '../../models/PARTIAL_3 #1.pth',\n",
    "    '../../models/PARTIAL_3 #2.pth',\n",
    "    '../../models/PARTIAL_3 #3.pth',\n",
    "    '../../models/PARTIAL_3 #4.pth'\n",
    "]\n",
    "\n",
    "PATH = '../../dataset/testing/same_cam/'\n",
    "DATATEST_PATH = PATH + 'testing.csv'\n",
    "IMAGES_PATH = PATH + '/images/full/'\n",
    "IMAGES_OCCL_PATH = PATH + '/images/occl_80/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_image2tensor(img, convert=True):\n",
    "    if convert:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = img[:, :, ::-1].transpose((2, 0, 1)).copy()\n",
    "    img = torch.from_numpy(img).float() / 255.0\n",
    "    img = [img]\n",
    "    img_tensors = torch.stack(img)\n",
    "\n",
    "    return img_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "for index, path in enumerate(MODEL_PATH):\n",
    "    data = {}\n",
    "    model  = bst.BstCnn()\n",
    "    checkpoint = ckp.load_checkpoint(load_dir=path)\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    model.eval()\n",
    "    \n",
    "    data['id'] = 'PART-' + str(index+1)\n",
    "    data['model'] = model\n",
    "    data['min_dist'] = checkpoint['dist'][0]\n",
    "    data['max_dist'] = checkpoint['dist'][1]\n",
    "    data['threshold'] = checkpoint['threshold'] + 0.1\n",
    "#     data['threshold'] = 0.3\n",
    "    data['threshold_list'] = checkpoint['threshold_list']\n",
    "    models.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.65\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(DATATEST_PATH)\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "verbose = False\n",
    "with torch.no_grad():\n",
    "    for index, data in df.iterrows():\n",
    "        if verbose == True:\n",
    "            print('No-{}'.format(index+1))\n",
    "            print('-'*50)\n",
    "        img1 = cv2.imread(IMAGES_PATH + data['image_1'])\n",
    "        img2 = cv2.imread(IMAGES_OCCL_PATH + data['image_2'])\n",
    "        label = data['label']\n",
    "\n",
    "        img1_part = list(create_partial.partial_image_2(img1))\n",
    "        img2_part = list(create_partial.partial_image_2(img2))\n",
    "\n",
    "        thresholds = []\n",
    "        dists = []\n",
    "        for i, (input1, input2, model) in enumerate(zip(img1_part, img2_part, models)):\n",
    "            input1 = cv_image2tensor(input1)\n",
    "            input2 = cv_image2tensor(input2)\n",
    "            out1, out2 = model['model'](input1, input2)\n",
    "            euclidean_distance = F.pairwise_distance(out1, out2)\n",
    "            \n",
    "            dist = metrics.normalize_data(euclidean_distance.item(), model['max_dist'])\n",
    "            dists.append(dist)\n",
    "            thresholds.append(model['threshold'])\n",
    "            \n",
    "            if verbose == True:\n",
    "                print('PART-{} DISTANCE => {}'.format((i+1), dist))\n",
    "                print('Threshold => {}'.format(model['threshold']))\n",
    "                print('-'*50)\n",
    "            \n",
    "        cat_dist, cat_thresh = metrics.concatenate(dists, thresholds, PROB_SAME['partial_1'])\n",
    "        \n",
    "        if verbose == True:\n",
    "            print('actual distance => {}'.format(cat_dist))\n",
    "            print('actual thresh => {}'.format(cat_thresh))\n",
    "            print('actual label => {}'.format(label))\n",
    "        \n",
    "        y_true.append(float(label))\n",
    "        pred = 0.0 if cat_dist <= cat_thresh else 1.0\n",
    "        y_pred.append(pred)\n",
    "        concatenated = torch.cat((cv_image2tensor(img1, False), cv_image2tensor(img2, False)),0)\n",
    "        \n",
    "        if verbose == True:\n",
    "            vis.imshow(torchvision.utils.make_grid(concatenated))\n",
    "            print('='*50, end='\\n\\n')\n",
    "\n",
    "acc = accuracy_score(np.array(y_true), np.array(y_pred))\n",
    "print('Accuracy : {}'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit",
   "language": "python",
   "name": "python37464biteb93521d834e45aeaba8e45a84cf6735"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
